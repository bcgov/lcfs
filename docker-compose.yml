services:
  db:
    container_name: db
    restart: "no"
    image: postgres:17
    environment:
      POSTGRES_DB: lcfs
      POSTGRES_USER: lcfs
      POSTGRES_PASSWORD: development_only
    ports:
      - 5432:5432
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - shared_network

  redis:
    container_name: redis
    restart: "no"
    image: redis:7.4.2
    command: redis-server --requirepass development_only
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - shared_network

  rabbitmq:
    image: rabbitmq:3-management
    container_name: rabbitmq
    environment:
      RABBITMQ_DEFAULT_USER: lcfs
      RABBITMQ_DEFAULT_PASS: development_only
      RABBITMQ_DEFAULT_VHOST: lcfs
    volumes:
      - ./docker/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
    ports:
      - "15672:15672" # Management UI
      - "5672:5672" # RabbitMQ Port
    networks:
      - shared_network

  #  clamav:
  #    image: clamav/clamav:stable_base
  #    volumes:
  #      - clamsocket:/var/run/clamav/
  #      - clamav:/var/lib/clamav
  #    ports:
  #      - "3310:3310"
  #    networks:
  #    - shared_network

  minio:
    image: minio/minio:latest
    restart: "no"
    command: "server --console-address :9001 --address :9000 /data"
    environment:
      - MINIO_ROOT_USER=s3_access_key
      - MINIO_ROOT_PASSWORD=development_only
      - MINIO_SERVER_URL=http://localhost:9000
    volumes:
      - s3:/data
    ports:
      - "9000:9000"
      - "9001:9001"
    networks:
      - shared_network

  create_bucket:
    image: minio/mc
    restart: "no"
    container_name: minio_init
    entrypoint: >
      /bin/sh -c "
      until mc alias set myminio http://minio:9000 s3_access_key development_only; do
        echo 'Waiting for MinIO...';
        sleep 5;
      done;
      mc mb myminio/lcfs
      "
    networks:
      - shared_network
    depends_on:
      - minio

  backend:
    container_name: backend
    restart: "no"
    build:
      context: ./backend
      dockerfile: "Dockerfile"
      target: dev
    volumes:
      - type: bind
        source: ./backend
        target: /app
        consistency: cached
    environment:
      LCFS_DB_HOST: db
      LCFS_DB_PORT: 5432
      LCFS_DB_USER: lcfs
      LCFS_DB_PASS: development_only
      LCFS_DB_BASE: lcfs
      LCFS_REDIS_HOST: redis
      LCFS_REDIS_PORT: 6379
      LCFS_REDIS_PASSWORD: development_only
      LCFS_RELOAD: true
      APP_ENVIRONMENT: dev
      LCFS_OLLAMA_URL: http://ollama:11434
      LCFS_RAG_SERVICE_URL: http://rag-llm:1416
      LCFS_CHAT_RAG_ENABLED: true
    ports:
      - "8000:8000" # Application port
      - "5678:5678" # Debugger port
    depends_on:
      - db
      - redis
      - create_bucket
      - rabbitmq
      # - clamav
    networks:
      - shared_network

  frontend:
    container_name: frontend
    restart: "no"
    build:
      dockerfile: Dockerfile.dev
      context: ./frontend
    command: npm run dev --force
    ports:
      - "3000:3000"
    volumes:
      - type: bind
        source: ./frontend
        target: /app
      - type: volume
        source: node_modules
        target: /app/node_modules
    networks:
      - shared_network

  # LCFS MCP Server - Development Only
  # Model Context Protocol server for local development and testing
  # WARNING: This service only runs in development environment
  mcp-server:
    container_name: lcfs-mcp-server
    restart: "no"
    build:
      context: ./mcp-server
      dockerfile: Dockerfile
    environment:
      APP_ENVIRONMENT: dev
      NODE_ENV: development
    volumes:
      - type: bind
        source: ./mcp-server/src
        target: /app/src
        consistency: cached
    networks:
      - shared_network
    # No ports exposed - uses stdio transport
    labels:
      - "lcfs.service=mcp-server"
      - "lcfs.environment=development-only"
      - "lcfs.description=Model Context Protocol server for local development"

  # Ollama service for LLM capabilities with streaming support
  ollama:
    image: ollama/ollama:latest
    container_name: lcfs-ollama
    restart: "no"
    profiles: ["rag"]
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
      - ./rag-system/scripts/pull-model.sh:/usr/local/bin/pull-model.sh:ro
    environment:
      OLLAMA_MODEL: "smollm2:135m"
      OLLAMA_KEEP_ALIVE: "5m"
      OLLAMA_NUM_PARALLEL: "4"
      OLLAMA_MAX_LOADED_MODELS: "2"
    entrypoint: ["/bin/bash", "/usr/local/bin/pull-model.sh"]
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 60s
    networks:
      - shared_network

  # Qdrant Vector Database for RAG system
  qdrant:
    image: qdrant/qdrant:v1.15.4
    container_name: lcfs-qdrant
    restart: "no"
    profiles: ["rag"]
    ports:
      - "6333:6333" # REST API + Web Dashboard
      - "6334:6334" # gRPC API
    volumes:
      - qdrant_storage:/qdrant/storage
    networks:
      - shared_network
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"
        reservations:
          memory: 256M
          cpus: "0.1"
    labels:
      - "lcfs.service=qdrant"
      - "lcfs.description=Vector database for LCFS RAG system"

  # RAG System - LLM API using Haystack + Hayhooks
  rag-llm:
    container_name: lcfs-rag-llm
    restart: "no"
    profiles: ["rag"]
    build:
      context: ./rag-system
      dockerfile: Dockerfile
    environment:
      HAYSTACK_TELEMETRY_ENABLED: "false"
      HF_HUB_DISABLE_TELEMETRY: "1"
      TOKENIZERS_PARALLELISM: "false"
      QDRANT_HOST: "qdrant"
      QDRANT_PORT: "6333"
      OLLAMA_URL: "http://ollama:11434"
      OLLAMA_MODEL: "smollm2:135m" # Use same model as Ollama
      EMBEDDING_MODEL: "BAAI/bge-small-en-v1.5" # Embedding model for document vectors
      RERANKER_MODEL: "cross-encoder/ms-marco-MiniLM-L-6-v2" # Reranker for result improvement
    ports:
      - "1416:1416"
    volumes:
      - huggingface_cache:/root/.cache/huggingface
      - type: bind
        source: ./rag-system/pipelines
        target: /opt/pipelines
        consistency: cached
      - type: bind
        source: ./rag-system/data
        target: /opt/data
        consistency: cached
    networks:
      - shared_network
    depends_on:
      - qdrant
      - ollama
    deploy:
      resources:
        limits:
          memory: 5G
          cpus: "1.0"
        reservations:
          memory: 2G
          cpus: "0.5"
    labels:
      - "lcfs.service=rag-llm"
      - "lcfs.description=LCFS RAG System LLM API using open-source models"

volumes:
  postgres_data:
    name: lcfs_postgres_data
  redis_data:
    name: lcfs_redis_data
  node_modules:
    name: lcfs_node_data
  s3:
    name: lcfs_s3_data
  clamav:
    name: clamav
  clamsocket:
    name: clamsocket
  ollama_models:
    name: lcfs_ollama_models
  huggingface_cache:
    name: lcfs_huggingface_cache
  qdrant_storage:
    name: lcfs_qdrant_storage

networks:
  shared_network:
    name: shared_network

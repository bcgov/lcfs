name: Testing pipeline

on:
  pull_request:
    branches:
      - '*'

permissions:
  contents: read
  actions: write
  checks: write
  pull-requests: write
  issues: write

jobs:
  backend-setup:
    name: 'Backend Setup'
    runs-on: ubuntu-latest
    outputs:
      # Add output to signal completion
      setup-complete: ${{ steps.setup-complete.outputs.complete }}
    services:
      postgres:
        image: postgres:17
        env:
          POSTGRES_DB: lcfs
          POSTGRES_USER: lcfs
          POSTGRES_PASSWORD: development_only
        ports:
          - 5432:5432
        # Add health check to ensure postgres is ready
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          sparse-checkout: |
            backend

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10.13'

      - name: Cache Poetry dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pypoetry
            backend/.venv
          key: ${{ runner.os }}-poetry-${{ hashFiles('backend/poetry.lock') }}-${{ hashFiles('backend/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-poetry-${{ hashFiles('backend/poetry.lock') }}-
            ${{ runner.os }}-poetry-

      - name: Install Poetry
        run: pip install poetry==1.6.1

      - name: Install backend dependencies
        run: |
          pip install --upgrade pip setuptools wheel
          cd backend
          poetry install

      - name: Wait for PostgreSQL to be ready
        run: |
          # Wait for PostgreSQL service to be fully ready using Docker
          CONTAINER_ID=$(docker ps -q -f "ancestor=postgres:17")
          until docker exec $CONTAINER_ID pg_isready -U lcfs; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done
          echo "PostgreSQL is ready!"

      - name: Run migrations and create template database
        run: |
          cd backend
          poetry run alembic upgrade heads
        env:
          LCFS_DB_HOST: localhost
          LCFS_DB_PORT: 5432
          LCFS_DB_USER: lcfs
          LCFS_DB_PASS: development_only
          LCFS_DB_BASE: lcfs

      - name: Create database dump with compatibility options
        run: |
          # Use Docker container's pg_dump to avoid version mismatch issues
          # Ubuntu runner has PostgreSQL 16 client tools, but container runs PostgreSQL 17
          CONTAINER_ID=$(docker ps -q -f "ancestor=postgres:17")

          if [ -z "$CONTAINER_ID" ]; then
            echo "Error: PostgreSQL container not found"
            exit 1
          fi

          # Run pg_dump inside the container to ensure version compatibility
          docker exec $CONTAINER_ID pg_dump -U lcfs -d lcfs \
            --no-owner \
            --no-privileges \
            --no-comments \
            --create \
            --clean \
            --if-exists \
            --format=custom \
            --compress=6 > lcfs_template.pgdump

          # Verify the dump was created and has content
          if [ ! -s lcfs_template.pgdump ]; then
            echo "Error: Database dump is empty or failed"
            exit 1
          fi

          echo "Database dump created successfully ($(stat --printf=%s lcfs_template.pgdump) bytes)"

      - name: Upload database template
        uses: actions/upload-artifact@v4
        with:
          name: db-template
          path: lcfs_template.pgdump
          retention-days: 1
          # Add compression to reduce size
          compression-level: 9

      - name: Mark setup as complete
        id: setup-complete
        run: echo "complete=true" >> $GITHUB_OUTPUT

  backend-tests:
    name: 'Backend Tests (Shard ${{ matrix.shard }}/${{ strategy.job-total }})'
    needs: backend-setup
    runs-on: ubuntu-latest
    # Add explicit dependency check
    if: needs.backend-setup.outputs.setup-complete == 'true'
    services:
      postgres:
        image: postgres:17
        env:
          POSTGRES_DB: lcfs
          POSTGRES_USER: lcfs
          POSTGRES_PASSWORD: development_only
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    strategy:
      matrix:
        shard:
          [
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
          ]
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          sparse-checkout: |
            backend

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10.13'

      - name: Cache Poetry dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pypoetry
            backend/.venv
          key: ${{ runner.os }}-poetry-${{ hashFiles('backend/poetry.lock') }}-${{ hashFiles('backend/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-poetry-${{ hashFiles('backend/poetry.lock') }}-
            ${{ runner.os }}-poetry-

      - name: Install Poetry
        run: pip install poetry==1.6.1

      - name: Install backend dependencies
        run: |
          pip install --upgrade pip setuptools wheel
          cd backend
          poetry install

      - name: Cache pytest timing data
        uses: actions/cache@v4
        with:
          path: backend/.pytest_cache
          key: ${{ runner.os }}-pytest-timing-${{ strategy.job-total }}-${{ hashFiles('backend/lcfs/tests/**/*.py') }}
          restore-keys: |
            ${{ runner.os }}-pytest-timing-${{ strategy.job-total }}-

      - name: Wait for PostgreSQL to be ready
        run: |
          # Wait for PostgreSQL service to be fully ready using Docker
          CONTAINER_ID=$(docker ps -q -f "ancestor=postgres:17")
          until docker exec $CONTAINER_ID pg_isready -U lcfs; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done
          echo "PostgreSQL is ready!"

      - name: Download database template
        uses: actions/download-artifact@v4
        with:
          name: db-template
          path: .

      - name: Verify database template
        run: |
          if [ ! -f lcfs_template.pgdump ]; then
            echo "Error: Database template file not found"
            exit 1
          fi

          if [ ! -s lcfs_template.pgdump ]; then
            echo "Error: Database template file is empty"
            exit 1
          fi

          echo "Database template verified ($(stat --printf=%s lcfs_template.pgdump) bytes)"

      - name: Setup shard-specific database
        run: |
          # Get the container ID of the postgres service
          CONTAINER_ID=$(docker ps -q -f "ancestor=postgres:17")

          # Create shard-specific database using Docker
          docker exec $CONTAINER_ID createdb -U lcfs lcfs_shard_${{ matrix.shard }}

          # Copy the dump file into the container
          docker cp lcfs_template.pgdump $CONTAINER_ID:/tmp/lcfs_template.pgdump

          # Restore using pg_restore inside the container to avoid version mismatch
          docker exec $CONTAINER_ID pg_restore \
            -U lcfs \
            -d lcfs_shard_${{ matrix.shard }} \
            --no-owner \
            --no-privileges \
            --disable-triggers \
            --single-transaction \
            --exit-on-error \
            --verbose \
            /tmp/lcfs_template.pgdump
        env:
          LCFS_DB_HOST: localhost
          LCFS_DB_PORT: 5432
          LCFS_DB_USER: lcfs
          LCFS_DB_PASS: development_only

      - name: Ensure pytest cache directory exists
        run: |
          cd backend
          mkdir -p .pytest_cache

      - name: Run backend tests for shard
        id: backend_tests
        run: |
          cd backend
          poetry run pytest --splits ${{ strategy.job-total }} --group ${{ matrix.shard }} --store-durations --maxfail=1 --tb=short --disable-warnings --quiet --junitxml=pytest-results-shard-${{ matrix.shard }}.xml || [ $? -eq 5 ]
        env:
          LCFS_DB_HOST: localhost
          LCFS_DB_PORT: 5432
          LCFS_DB_USER: lcfs
          LCFS_DB_PASS: development_only
          LCFS_DB_BASE: lcfs_shard_${{ matrix.shard }}
          LCFS_REDIS_HOST: localhost
          LCFS_REDIS_PORT: 6379
          LCFS_REDIS_PASSWORD: development_only
          APP_ENVIRONMENT: dev
          LCFS_CHES_CLIENT_ID: mock_client_id
          LCFS_CHES_CLIENT_SECRET: mock_client_secret
          LCFS_CHES_AUTH_URL: http://mock_auth_url
          LCFS_CHES_SENDER_EMAIL: noreply@gov.bc.ca
          LCFS_CHES_SENDER_NAME: Mock Notification System
          LCFS_CHES_EMAIL_URL: http://mock_email_url

      - name: Upload pytest results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pytest-results-shard-${{ matrix.shard }}
          path: backend/pytest-results-shard-${{ matrix.shard }}.xml
          retention-days: 1

  post-backend-tests:
    name: 'Publish Backend Test Results'
    if: always()
    needs: backend-tests
    runs-on: ubuntu-latest
    steps:
      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          pattern: pytest-results-shard-*
          merge-multiple: true

      - name: Merge test results
        run: |
          pip install junitparser
          cat > merge_results.py << 'EOF'
          import glob
          from junitparser import JUnitXml

          suites = []
          for file in glob.glob('pytest-results-shard-*.xml'):
              suites.append(JUnitXml.fromfile(file))

          merged = JUnitXml()
          for suite in suites:
              for testsuite in suite:
                  merged.add_testsuite(testsuite)

          merged.write('merged-pytest-results.xml')
          EOF
          python3 merge_results.py

      - name: Publish Backend Test Results
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          files: merged-pytest-results.xml
          github_token: ${{ secrets.GITHUB_TOKEN }}
          comment_title: 'Backend Test Results'
          check_name: 'Backend Test Results'
          fail_on: 'errors'
          report_individual_runs: 'true'
          deduplicate_classes_by_file_name: 'true'

  frontend-setup:
    name: 'Frontend Setup'
    runs-on: ubuntu-latest
    outputs:
      setup-complete: ${{ steps.setup-complete.outputs.complete }}
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          sparse-checkout: |
            frontend

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Cache node_modules
        uses: actions/cache@v4
        with:
          path: frontend/node_modules
          key: ${{ runner.os }}-node-modules-${{ hashFiles('frontend/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-modules-

      - name: Install frontend dependencies
        run: |
          cd frontend
          npm ci --prefer-offline --no-audit --no-fund

      # - name: Type Check Frontend (parallel)
      #   run: |
      #     cd frontend
      #     npm run type-check &
      #     TYPE_CHECK_PID=$!
      #     wait $TYPE_CHECK_PID

      - name: Cache node_modules for test shards
        uses: actions/cache@v4
        with:
          path: frontend/node_modules
          key: ${{ runner.os }}-node-modules-shared-${{ hashFiles('frontend/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-modules-shared-

      - name: Mark setup as complete
        id: setup-complete
        run: echo "complete=true" >> $GITHUB_OUTPUT

  frontend-tests:
    name: 'Frontend Tests (Shard ${{ matrix.shardIndex }}/${{ matrix.shardTotal }})'
    needs: frontend-setup
    runs-on: ubuntu-latest
    if: needs.frontend-setup.outputs.setup-complete == 'true'
    strategy:
      matrix:
        shardIndex:
          [
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
          ]
        shardTotal: [32]
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          sparse-checkout: |
            frontend

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Restore node_modules cache
        uses: actions/cache@v4
        with:
          path: frontend/node_modules
          key: ${{ runner.os }}-node-modules-shared-${{ hashFiles('frontend/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-modules-shared-
            ${{ runner.os }}-node-modules-

      - name: Install frontend dependencies (fallback)
        run: |
          cd frontend
          if [ ! -d "node_modules" ]; then
            npm ci --prefer-offline --no-audit --no-fund
          fi

      - name: Run frontend tests
        id: frontend_tests
        run: |
          cd frontend
          echo "=== Running tests for shard ${{ matrix.shardIndex }}/${{ matrix.shardTotal }} ==="

          # Run tests and capture exit code
          set +e
          npm run test:ci:memory
          EXIT_CODE=$?
          set -e

          # Handle exit codes appropriately
          if [ $EXIT_CODE -eq 0 ]; then
            echo "✅ Tests completed successfully"
          elif [ $EXIT_CODE -eq 1 ]; then
            # Check if this is due to no test files (empty shard) or real failure
            echo "Checking if shard ${{ matrix.shardIndex }} is empty..."
            
            # Try to determine if this shard should have test files
            if SHARD_INDEX=${{ matrix.shardIndex }} SHARD_TOTAL=${{ matrix.shardTotal }} node --max-old-space-size=8192 ./node_modules/.bin/vitest run --reporter=verbose --shard=${{ matrix.shardIndex }}/${{ matrix.shardTotal }} --run=false 2>&1 | grep -q "No test files found"; then
              echo "✅ Empty shard detected - this is expected for shard ${{ matrix.shardIndex }} with 195 total test files"
              
              # For empty shards, we'll let the blob report merging step handle the missing report
              # The merge step has logic to handle missing blob reports gracefully
              echo "✅ Empty shard handled successfully - no blob report needed"
            else
              echo "❌ Real test failure detected in shard ${{ matrix.shardIndex }}"
              exit $EXIT_CODE
            fi
          else
            echo "❌ Tests failed with unexpected exit code $EXIT_CODE"
            exit $EXIT_CODE
          fi
        env:
          CI: true
          SHARD_INDEX: ${{ matrix.shardIndex }}
          SHARD_TOTAL: ${{ matrix.shardTotal }}
          VITEST_POOL_THREADS: '4'

      - name: Debug test failure details
        if: failure()
        run: |
          cd frontend
          echo "=== FRONTEND TEST FAILURE DEBUG ==="
          echo "Failed shard: ${{ matrix.shardIndex }}/${{ matrix.shardTotal }}"
          echo "Memory settings: 8192MB (npm script)"
          echo "Environment: CI=$CI"
          echo ""
          echo "Getting detailed error information..."
          SHARD_INDEX=${{ matrix.shardIndex }} SHARD_TOTAL=${{ matrix.shardTotal }} node --max-old-space-size=8192 ./node_modules/.bin/vitest run --reporter=verbose --shard=${{ matrix.shardIndex }}/${{ matrix.shardTotal }} || true
        env:
          CI: true

      - name: Upload blob report to GitHub Actions Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: blob-report-${{ matrix.shardIndex }}
          path: frontend/.vitest-reports/*
          include-hidden-files: true
          retention-days: 1
          compression-level: 9

  post-frontend-tests:
    name: 'Publish Frontend Test Results'
    if: always()
    needs: [frontend-tests]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          sparse-checkout: |
            frontend

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Install Vitest
        run: |
          cd frontend
          VITEST_VERSION=$(node -p "require('./package.json').devDependencies.vitest")
          echo "Installing vitest version: $VITEST_VERSION"
          npm install vitest@$VITEST_VERSION --no-save

      - name: Download blob reports from GitHub Actions Artifacts
        uses: actions/download-artifact@v4
        with:
          path: frontend/.vitest-reports
          pattern: blob-report-*
          merge-multiple: true

      - name: Merge reports
        run: |
          cd frontend

          # Check if we have any blob reports
          if [ ! -d ".vitest-reports" ] || [ -z "$(find .vitest-reports -name 'blob-*.json' -type f 2>/dev/null)" ]; then
            echo "No blob reports found, creating empty test results file"
            mkdir -p .vitest-reports
            echo '<?xml version="1.0" encoding="UTF-8"?><testsuite name="frontend" tests="0" failures="0" errors="0" skipped="0"></testsuite>' > .vitest-reports/vitest-results.xml
            exit 0
          fi

          # Validate blob reports and remove invalid ones
          echo "Validating blob reports..."
          for file in .vitest-reports/blob-*.json; do
            if [ -f "$file" ]; then
              if ! node -pe "JSON.parse(require('fs').readFileSync('$file', 'utf8')); 'valid'" 2>/dev/null | grep -q "valid"; then
                echo "Removing invalid blob report: $file"
                rm "$file"
              else
                echo "Valid blob report: $file"
              fi
            fi
          done

          # Check again if we have valid reports
          if [ -z "$(find .vitest-reports -name 'blob-*.json' -type f 2>/dev/null)" ]; then
            echo "No valid blob reports found after validation, creating empty test results file"
            echo '<?xml version="1.0" encoding="UTF-8"?><testsuite name="frontend" tests="0" failures="0" errors="0" skipped="0"></testsuite>' > .vitest-reports/vitest-results.xml
            exit 0
          fi

          # Merge valid reports
          echo "Merging valid blob reports..."
          npx --yes vitest@$(node -p "require('./package.json').devDependencies.vitest || require('./package.json').dependencies.vitest") --merge-reports --reporter=junit --outputFile=.vitest-reports/vitest-results.xml

      - name: Publish Frontend Test Results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: frontend/.vitest-reports/vitest-results.xml
          github_token: ${{ secrets.GITHUB_TOKEN }}
          comment_title: 'Frontend Test Results'
          check_name: 'Frontend Test Results'
          fail_on: 'errors'
          report_individual_runs: 'true'
          deduplicate_classes_by_file_name: 'true'
          comment_mode: 'always'

  teams-notification:
    needs: [post-backend-tests, post-frontend-tests]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Determine Test Status
        id: test_status
        run: |
          if [[ "${{ contains(needs.post-backend-tests.result, 'failure') }}" == "true" || "${{ contains(needs.post-frontend-tests.result, 'failure') }}" == "true" ]]; then
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "theme_color=FF0000" >> $GITHUB_OUTPUT
          else
            echo "status=success" >> $GITHUB_OUTPUT
            echo "theme_color=00FF00" >> $GITHUB_OUTPUT
          fi

      - name: Send custom Teams notification
        run: |
          # Replace double quotes with single quotes in PR title to prevent JSON issues
          PR_TITLE_RAW="${{ github.event.pull_request.title }}"
          PR_TITLE="${PR_TITLE_RAW//\"/\'}"
          PR_URL="${{ github.event.pull_request.html_url }}"
          REPO="${{ github.repository }}"
          PR_NUMBER="${{ github.event.pull_request.number }}"
          PR_AUTHOR="${{ github.event.pull_request.user.login }}"
          COMMIT_SHA="${{ github.event.pull_request.head.sha }}"
          AVATAR_URL="${{ github.event.pull_request.user.avatar_url }}"
          THREAD_ID="pr-${PR_NUMBER}"
          WORKFLOW_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          FRONTEND_SUMMARY="${{ steps.test_status.outputs.frontend_summary }}"
          MESSAGE="**Test Results**\n- Backend Tests: ${{ needs.post-backend-tests.result }}\n- Frontend Tests: ${{ needs.post-frontend-tests.result }}\n\n**Status:** $([[ '${{ steps.test_status.outputs.status }}' == 'failure' ]] && echo '❌ Failed' || echo '✅ Passed')"
          COLOR="${{ steps.test_status.outputs.theme_color }}"

          cat <<EOF > teams_card.json
          {
            "@type": "MessageCard",
            "@context": "http://schema.org/extensions",
            "themeColor": "$COLOR",
            "summary": "GitHub Notification for PR #$PR_NUMBER",
            "sections": [
              {
                "activityTitle": "$PR_TITLE - PR #$PR_NUMBER",
                "activitySubtitle": "Repository: $REPO, Author: $PR_AUTHOR",
                "activityImage": "$AVATAR_URL",
                "facts": [
                  { "name": "Repository", "value": "$REPO" },
                  { "name": "PR Title", "value": "$PR_TITLE" },
                  { "name": "Branch", "value": "${{ github.head_ref }}" },
                  { "name": "Commit", "value": "$COMMIT_SHA" },
                ],
                "text": "$MESSAGE",
                "markdown": true
              }
            ],
            "replyToId": "$THREAD_ID",
            "potentialAction": [
              {
                "@type": "OpenUri",
                "name": "View Pull Request",
                "targets": [
                  {
                    "os": "default",
                    "uri": "$PR_URL"
                  }
                ]
              },
              {
                "@type": "OpenUri",
                "name": "View Workflow Run",
                "targets": [
                  {
                    "os": "default",
                    "uri": "$WORKFLOW_URL"
                  }
                ]
              }
            ]
          }
          EOF
          curl -H "Content-Type: application/json" -d @teams_card.json "${{ secrets.TEAMS_WEBHOOK_URL }}"
